# PCA-from-VCF-files
This project demonstrates how to perform principal component analysis (PCA) using an aggregated VCF file or multiple non-aggregated VCF files.

# Principal Component Analysis (PCA) from VCF files

HELLO!
This document illustrates the process for performing a Principal Component Analysis (PCA) starting from a VCF file. It is necessary for the VCF file to contain the `GT` (genotype) field, as other fields are not considered for PCA.

PCA is a simple yet powerful clustering tool based on the decomposition of the data matrix. In our case, rows represent variants, and columns represent the genotypes of our samples. The goal is to find eigenvectors and eigenvalues.

Eigenvalues are sorted from largest to smallest. In short, the first eigenvalue indicates our first Principal Component (PC), and its direction in the PC space is given by the corresponding eigenvector. (This is an intentionally simplified and brief explanation).

Through this process, we are able to identify a lower-dimensional subspace, or an equivalent space, that "better" describes the distribution of our data.

## Prerequisites and Tools
To run this pipeline, you will need:
- PLINK version 1.9 (either [installed directly](https://www.cog-genomics.org/plink/) or via a [Docker container](https://github.com/asherkhb/plink-docker) ).
- BCFtools installed (either installed directly or via a Docker container [https://github.com/samtools/bcftools](url)).
- Two chromosome naming convention files:
    - The first file (in our case, named `no_chr_name_convention.txt`) must contain two columns: the first column with chromosome names like `chrN` (for N = 1, ...22, X, Y) and the second column with the corresponding `N` (for N = 1, ...22, X, Y).
      ```text
        chr1     1
        chr2     2
        chr3     3
        chr4     4
        chr5     5
        chr6     6
        chr7     7
        chr8     8
        chr9     9
        chr10    10
        chr11    11
        chr12    12
        chr13    13
        chr14    14
        chr15    15
        chr16    16
        chr17    17
        chr18    18
        chr19    19
        chr20    20
        chr21    21
        chr22    22
        chrX     X
        ```
    - The second file (in our case, named `chr_name_conv.txt`) must also contain two columns, but with the order inverted compared to the first file (i.e., `N` in the first column and `chrN` in the second).
        ```text
        1        chr1
        2        chr2
        3        chr3
        4        chr4
        5        chr5
        6        chr6
        7        chr7
        8        chr8
        9        chr9
        10       chr10
        11       chr11
        12       chr12
        13       chr13
        14       chr14
        15       chr15
        16       chr16
        17       chr17
        18       chr18
        19       chr19
        20       chr20
        21       chr21
        22       chr22
        23       chrX
        24       chrY
        25       chrXY
        26       chrMT
        ```
- Reference genome FASTA (.fa) and its index (.fai) for the chosen build (e.g., hg19/hg38), available from here ([hg19](https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/)/[hg38](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/)).


> **NOTE on PLINK in our case study**
> `Plink` accepts VCF files as input (which must be accompanied by their respective `.tbi` index file). It is crucial that the VCF file contains the `GT` (genotype) field for each sample.
> Our input file will be a `vcf.gz` with its `.tbi` index file. In this particular case study, it is assumed that the `CHROM` column contains chromosomes denoted as "`N`" and the `ID` column is formatted as "`CHROM:POS:REF:ALT`". However, modifying the `ID` column or the `CHROM`column is not strictly necessary for PCA, as `plink` does not use it directly for this analysis.
>
> The output files generated by `plink` for PCA will be three: a `.log` file, an `.eigenvec` (containing eigenvectors) file and an `.eigenval` file (containing eigenvalues).
> 
> From these files, dedicated pipelines can be used to analyze the explained variance of the data using the `.eigenval` file, or plot the different PCs using the `.eigenvec` file.

## Optimizing Input Files: Working with Multiple Cohorts

Frequently, in studies, the need arises to compare multiple cohorts. For example, one might want to compare data from patients belonging to a "mesothelioma" cohort with data for the same pathology but originating from a different laboratory.

> **NOTE**
> If your only need is to merge two VCF files belonging to different cohorts, normalizing them, you can skip directly to step **3) Merging Cohorts**.

---

### 1) Merging VCF Files for a Single Cohort

If the data for a single cohort are distributed across multiple VCF files, they need to be merged.

With `bcftools`, you can merge all VCF files for your cohort. First, create a text file (e.g., `list_vcf.list`) that lists the full paths of all VCF files to be merged, one per line.

Example line in the `.list` file:
`path/to/file/sample1.vcf.gz`

Next, perform the merge:

```bash
bcftools merge --threads 64 -l list_vcf.list -Oz -o /path/to/merged_file_cohort1.vcf.gz
````

  - It is good practice to generate statistics with `bcftools stats` to verify the integrity of the generated file:

    ```bash
    /path/to/bcftools stats merged_file_cohort1.vcf.gz > /path/to/merged_file_cohort1.stat
    ```

    The file to inspect is `merged_file_cohort1.stat`.

#### NOTE

It is very important to split multi-allelic records. Often, alternative alleles can be represented as `<NON_REF>`, depending on how the VCFs were generated. Therefore, it is good practice to:

1. Split multi-allelic variants
2. Remove those that have `<NON_REF>` as an ALT allele
3. And correct possible REF/ALT discrepancies with respect to the reference genome

This can be done with the following commands:

**Splitting multi-allelic variants**: This is a recommended practice to separate records containing multiple alternative alleles. The goal is to have variants uniquely defined by `CHROM:POS:REF:ALT`.

    ```bash
    bcftools norm -m-any --check-ref w -f /path/to/hg19.fa # or hg38.fa \
    /path/to/merged_file_cohort1.vcf.gz -Oz -o /path/to/splt_merged_file_cohort1.vcf.gz
    ```
Where `hg19.fa` or `hg38.fa` is the reference genome used. Note that it is common for references like hg19 to have chromosomes named with `N` instead of `chrN`. 
Always check the specifications of the tool and the reference genome. Generally, these files (e.g., `hg19.fa`, `hg38.fa`) **must be** in the same directory as their index files (`.fai`).

***Remove the `<NON_REF>` as an ALT allele and correct possible REF/ALT discrepancies**

```bash
bcftools +fixref /path/to/splt_merged_file_cohort1.vcf.gz -Ou -- -f/path/to/hg19.fa # or hg38.fa --mode flip --discard | \
bcftools view -e 'ALT="<NON_REF>"' -Oz -o /path/to/fixref_splt_merged_file_cohort1.vcf.gz
```


> #### NOTE
>
> Step 2) is specific to particular normalization needs (like those mentioned in the NIG case study). If the `CHROM` and `ID` columns are already consistent, you can proceed directly to step 3).
-----


### 2) Normalizing VCF File Headers (Specific to a Case Study)

At this point, we might want to modify the VCF file headers so that the `CHROM` column uses the `N` notation (instead of `chrN`) and the `ID` column contains identifiers in the `CHROM:POS:REF:ALT` format instead of, for example, `rs` identifiers.
For these steps, the use of a mapping files `no_chr_name_convention.txt` and `chr_name_conv.txt` is assumed.

* **Creating the `ID` column in `CHROM:POS:REF:ALT` format**:

    ```bash
    bcftools annotate --rename-chrs /path/to/chr_name_conv.txt \
    /path/to/fixref_splt_merged_file_cohort1.vcf.gz | \
    bcftools norm -Ou -f /path/to/hg19.fa # or hg38.fa | \
    bcftools annotate -Oz -x ID -I +'%CHROM:%POS:%REF:%ALT' \
    -o /path/to/newID_fixref_splt_merged_file_cohort1.vcf.gz
    ```

  * **Changing the `CHROM` column nomenclature from `chrN` to `N`**:
   

    ```bash
    bcftools annotate --rename-chrs/path/to/no_chr_name_convention.txt \
    /path/to/newID_fixref_splt_merged_file_cohort1.vcf.gz \
    -Oz -o /path/to/noCHR_newID_fixref_splt_merged_file_cohort1.vcf.gz
    ```

-----

### 3) Merging Cohorts

To merge two VCF files (e.g., one per cohort, already preprocessed as described above), `bcftools merge` is the recommended tool.

> NOTE: the `vcf.gz` files must be aligned to the same reference genome hg38 or hg19.

```bash
bcftools merge --threads 64 \
/path/to/noCHR_newID_fixref_splt_merged_file_cohort1.vcf.gz \
/path/to/noCHR_newID_fixref_splt_merged_file_cohort2.vcf.gz \
-Oz -o /path/to/merged_cohorts.vcf.gz
```

**Expected number of variants in the merged file**:

  * The total number of variants in the `merged_cohorts.vcf.gz` file will be: (Number of unique variants in file 1) + (Number of unique variants in file 2) + (Number of variants common to both files).
  * There will be no duplication of rows for common variants (same position, REF, and ALT). These will be merged into a single record containing sample data from both files.

-----

### 4) Recommended Additional Checks

It is good practice to perform some checks on the final merged VCF file:

  * **Verify the number of variants**: Ensure it matches expectations (common variants + unique variants from both files).

    ```bash
    zgrep -v "#" /path/to/merged_cohorts.vcf.gz | wc -l
    ```

    Run the same command on the original files for comparison.

  * **Recreate a statistics file**:

    ```bash
    bcftools stats /path/to/merged_cohorts.vcf.gz > /path/to/merged_cohorts.stat
    ```

  * **Check for any REF/ALT flips post-merge**:
    The `bcftools +fixref` command may require chromosomes to be in `chrN` format. If the merged file uses the `N` notation, temporary reconversion is necessary.

      - Rename chromosomes from `N` to `chrN` (using a mapping file like `chr_name_conv.txt`):

        ```bash
        bcftools annotate --rename-chrs /path/to/chr_name_conv.txt \
        /path/to/merged_cohorts.vcf.gz \
        -o /path/to/CHR_merged_cohorts.vcf.gz
        ```

## 5) **OPTIONAL** Use `fixref` to check and correct any flips:

    ```bash
    bcftools +fixref /path/to/CHR_merged_cohorts.vcf.gz \
    -Oz -o /path/to/FIXREF_CHR_merged_cohorts.vcf.gz \
    -- -f /path/to/hg19.fa # or hg38.fa
    --mode flip --discard
    ```

-----

## 6) **OPTIONAL** Extracting Selected Variants

If you have a list of selected variants on which you want to perform PCA, and therefore you need to work with only a subset of the variants present in your `vcf.gz` file, execute the following command:

```bash
bcftools view --include 'ID=@variant_list.txt' \
    /path/to/CHR_merged_cohorts.vcf.gz \
    -Oz -o /path/to/VAR_selected_CHR_merged_cohorts.vcf.gz
```

**Note:** In this particular case, because we want to extract variants using only the ID field (which should be formatted as `CHROM:POS:REF:ALT`), the `variant_list.txt` file must be a single-column file. Each row in this file should contain a distinct variant ID in the following format:`CHROM:POS:REF:ALT`


## 7) Running PCA with Plink

Once the final merged and normalized VCF file (e.g., `merged_cohorts.vcf.gz` or `FIXREF_CHR_merged_cohorts.vcf.gz` or  `VAR_selected_CHR_merged_cohorts.vcf.gz`) is obtained, PCA can be performed.

Launch the Docker container with `plink`:

```bash
plink --vcf /path/to/merged_cohorts.vcf.gz --pca --double-id --out merged_cohorts
```

Options used:

  * `--vcf`: Specifies the input VCF file.
  * `--pca`: Performs principal component analysis.
  * `--double-id`: Ensures FID and IID are preserved.
  * `--out`: Prefix for output files (`merged_cohorts.eigenvec`, `merged_cohorts.eigenval`, `merged_cohorts.log`).

-----

## Other (Optional): Selecting Only Common Variants between VCFs

If the goal is to keep only the variants present in *both* VCF files before merging (or as a separate analysis):

  * One can use `bcftools isec` (intersection). This command is useful for identifying and keeping only variants common to two or more datasets.

    ```bash
    bcftools isec -n=2 -Oz -o merged_common_variants.vcf.gz \
    /path/to/noCHR_newID_fixref_splt_merged_file_cohort1.vcf.gz \
    /path/to/noCHR_newID_fixref_splt_merged_file_cohort2.vcf.gz
    ```

    The `-n=2` option specifies that a variant must be present in both input files to be included in the `merged_common_variants.vcf.gz` output.

<!-- end list -->

```
```

You are absolutely right. My apologies—I completely missed that the guide was in English and responded in Italian based on our conversation. Thank you for the correction and for providing the file again.

Here is the new section written in English, formatted to be perfectly consistent with the rest of your README file. You can copy and paste this directly into your document.

-----

<br>

## 8) (Optional but Recommended) Outlier Identification and Removal

After performing PCA, a crucial step is to identify **outliers**—samples that are genetically distant from the main clusters. These samples could represent individuals of a different ancestry, sample contamination, or technical artifacts. Removing them is important as they can skew the principal components and affect downstream analyses.

A first approach is to visually inspect a scatter plot of the first two principal components (PC1 vs. PC2). However, for a more robust and multidimensional identification, we can use the **Mahalanobis distance**.

This metric measures the distance of each sample from the center of the data's distribution, taking into account the covariance between the principal components. This allows us to evaluate a sample based on multiple PCs simultaneously (e.g., the first 10), providing a more reliable method for outlier detection.

### Implementation with R

The following R script can be used to identify outliers from the `.eigenvec` file generated by PLINK. The script performs the following steps:

1.  Loads the eigenvector file.
2.  Calculates the Mahalanobis distance for each sample using the first 10 PCs.
3.  Sets a threshold based on the Chi-squared distribution (using the 0.999 quantile and 10 degrees of freedom).
4.  Flags any sample exceeding the threshold as an outlier.
5.  Generates a PC1 vs. PC2 plot highlighting the outliers.
6.  Creates a text file (`outliers_to_remove.txt`) containing the IDs of the samples to be removed.

#### R Prerequisites

Ensure you have the `MASS` and `ggplot2` libraries installed:

```r
install.packages("MASS")
install.packages("ggplot2")
```

#### R Code for Outlier Detection

```r
# Load required libraries
library(MASS)
library(ggplot2)

# Specify the path to your .eigenvec file
eigenvectors <- "merged_cohorts.eigenvec" # <-- EDIT WITH YOUR FILE PATH
Note: Plink does not write a header you should create one

# --- Outlier Analysis ---

# Select the first 10 Principal Components for the analysis
pcs_to_use <- 10
pcs <- as.matrix(eigenvectors[, paste0("PC", 1:pcs_to_use)])

# Calculate the center (mean) and covariance matrix of the PCs
center <- colMeans(pcs)
covmat <- cov(pcs)

# Calculate the Mahalanobis distance for each sample
mahal <- mahalanobis(pcs, center, covmat)

# Set the threshold for defining an outlier.
# We use the 0.999 quantile of the Chi-squared distribution.
# The degrees of freedom (df) equal the number of PCs used.
threshold <- qchisq(0.999, df = pcs_to_use)

# Add a boolean column (TRUE/FALSE) to flag outliers
eigenvectors$is_outlier <- mahal > threshold

# Print the number of identified outliers to the console
cat("Number of outliers detected:", sum(eigenvectors$is_outlier), "\n")

# Create a scatter plot of PC1 vs PC2
# Outlier samples will be colored in red
ggplot(eigenvectors, aes(x = PC1, y = PC2, color = is_outlier)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"), name = "Is Outlier?") +
  theme_minimal() +
  labs(title = "Outlier Detection using Mahalanobis Distance",
       subtitle = paste(sum(eigenvectors$is_outlier), "outliers detected"),
       x = "Principal Component 1 (PC1)",
       y = "Principal Component 2 (PC2)") +
  coord_fixed()

# Extract the FID and IID of the outlier samples
# Plink's --remove flag requires a two-column file (FID and IID) without a header
outliers_to_remove <- eigenvectors[eigenvectors$is_outlier, c("FID", "IID")]

# Write the file that Plink will use for removal
output_file <- "outliers_to_remove.txt"
write.table(outliers_to_remove,
            file = output_file,
            sep = "\t", 
            row.names = FALSE, 
            col.names = FALSE, 
            quote = FALSE)

cat("File '", output_file, "' created with", nrow(outliers_to_remove), "samples to remove.\n")
```

